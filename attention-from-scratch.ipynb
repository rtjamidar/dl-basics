{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-08T09:35:24.630972Z"
    },
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/ratneshjamidar/miniconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - cython\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    cython-0.29.35             |  py310h313beb8_0         2.1 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         2.1 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  cython             pkgs/main/osx-arm64::cython-0.29.35-py310h313beb8_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "                                                                                \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "# !conda install -y pytorch torchvision torchaudio -c pytorch-nightly\n",
    "# !conda install -y mpmath\n",
    "# !conda install -y cython "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T09:03:44.553297Z",
     "start_time": "2023-07-08T09:03:44.547576Z"
    }
   },
   "source": [
    "# Reference: https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T09:15:40.693034Z",
     "start_time": "2023-07-08T09:15:40.681953Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Always': 0, 'curious': 1, 'my': 2, 'son': 3, 'stay': 4}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Always stay curious my son\"\n",
    "vocab = {word: i for i, word in enumerate(sorted(sentence.split(\" \")))}\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dumb Tokenizer, for every word it will give an integer, let's create a tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T09:15:40.864592Z",
     "start_time": "2023-07-08T09:15:40.692894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 4, 1, 2, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "tensor = torch.tensor([vocab[word] for word in sentence.split(\" \")])\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have sentence vector, we can now prepare vector embedding, we will pick embedding dimension to be `32`\n",
    "We will do random initialisation of embedding of size `5*32` , since our tensor size is `5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 32])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(33)\n",
    "\n",
    "#effectively a look table for each vector, it usually have size more than sentence length, equals to vocab size\n",
    "embed = torch.nn.Embedding(5, 32)\n",
    "embedded_sentence = embed(tensor).detach()\n",
    "embedded_sentence.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will setup key, query and value matrices W<sub>k</sub>, W<sub>q</sub> and W<sub>v</sub>\n",
    "\n",
    "Dimensions of  W<sub>k</sub> and  W<sub>q</sub> is  d<sub>k</sub>*d ,  d<sub>q</sub>*d,  d<sub>v</sub>*d\n",
    "where q==k and d = dimension of token vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([36, 32])\n",
      "torch.Size([36, 32])\n",
      "torch.Size([48, 32])\n"
     ]
    }
   ],
   "source": [
    "d = embedded_sentence.shape[1]\n",
    "q, k, v = 36, 36, 48\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(q,d))\n",
    "W_key = torch.nn.Parameter(torch.rand(k,d))\n",
    "W_value = torch.nn.Parameter(torch.rand(v,d))\n",
    "\n",
    "print(W_query.shape)\n",
    "print(W_key.shape)\n",
    "print(W_value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 36])\n",
      "torch.Size([5, 36])\n",
      "torch.Size([5, 48])\n"
     ]
    }
   ],
   "source": [
    "queries = torch.matmul(W_query,embedded_sentence.T).T\n",
    "keys = torch.matmul(W_query,embedded_sentence.T).T\n",
    "values = torch.matmul(W_value,embedded_sentence.T).T\n",
    "\n",
    "print(queries.shape)\n",
    "print(keys.shape)\n",
    "\n",
    "print(values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n"
     ]
    }
   ],
   "source": [
    "# unnormalize attention weights\n",
    "\n",
    "omega =  queries.matmul(keys.T)\n",
    "print(omega.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "attention_weights = F.softmax(omega/k**(0.5), dim=0)\n",
    "print(attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 48])\n"
     ]
    }
   ],
   "source": [
    "context_vector = attention_weights.matmul(values)\n",
    "print(context_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's extend this to multihead attention now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded_sentence_multi_head shape torch.Size([3, 32, 5])\n",
      "keys_multi_head shape torch.Size([3, 36, 5])\n",
      "queries_multi_head shape torch.Size([3, 36, 5])\n",
      "values_multi_head shape torch.Size([3, 48, 5])\n",
      "omega_multi_head shape torch.Size([3, 5, 5])\n",
      "context_vector shape torch.Size([3, 5, 48])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bl/zr7m9n4959ndslll4rt_m6q00000gp/T/ipykernel_99553/1792107392.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  context_vector = torch.bmm(F.softmax(omega_multi_head/k**0.5),values_multi_head.transpose(-1,-2))\n"
     ]
    }
   ],
   "source": [
    "h = 3\n",
    "\n",
    "embedded_sentence_multi_head = embedded_sentence.T.repeat(3,1,1)\n",
    "\n",
    "print(\"embedded_sentence_multi_head shape {}\".format(embedded_sentence_multi_head.shape) )\n",
    "W_query_multi_head = torch.nn.Parameter(torch.rand(h, q, d))\n",
    "W_key_multi_head = torch.nn.Parameter(torch.rand(h, k, d))\n",
    "W_value_multi_head = torch.nn.Parameter(torch.rand(h, v, d))\n",
    "\n",
    "keys_multi_head = torch.bmm(W_key_multi_head, embedded_sentence_multi_head)\n",
    "queries_multi_head = torch.bmm(W_query_multi_head, embedded_sentence_multi_head)\n",
    "values_multi_head = torch.bmm(W_value_multi_head,embedded_sentence_multi_head)\n",
    "print(\"keys_multi_head shape {}\".format(keys_multi_head.shape) )\n",
    "print(\"queries_multi_head shape {}\".format(queries_multi_head.shape) )\n",
    "print(\"values_multi_head shape {}\".format(values_multi_head.shape) )\n",
    "\n",
    "\n",
    "omega_multi_head = torch.bmm(keys_multi_head.transpose(-2,-1),queries_multi_head)\n",
    "\n",
    "print(\"omega_multi_head shape {}\".format(omega_multi_head.shape) )\n",
    "\n",
    "context_vector = torch.bmm(F.softmax(omega_multi_head/k**0.5),values_multi_head.transpose(-1,-2))\n",
    "print(\"context_vector shape {}\".format(context_vector.shape) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_cross_attention shape torch.Size([36, 16])\n",
      "context_vector_cross_attention shape torch.Size([16, 48])\n"
     ]
    }
   ],
   "source": [
    "# this is decoder input\n",
    "embedded_sentence_2 = torch.rand(16, 32)\n",
    "\n",
    "#embedded_sentence is encoder output \n",
    "\n",
    "\n",
    "W_key_cross_attention = torch.nn.Parameter(torch.rand(k, d))\n",
    "W_value_cross_attention = torch.nn.Parameter(torch.rand(v, d))\n",
    "W_query_cross_attention = torch.nn.Parameter(torch.rand(q, d))\n",
    "\n",
    "keys_cross_attention = torch.matmul(W_key_cross_attention,embedded_sentence.T )\n",
    "values_cross_attention = torch.matmul(W_value_cross_attention,embedded_sentence.T )\n",
    "\n",
    "query_cross_attention =  torch.matmul(W_query_cross_attention,embedded_sentence_2.T )\n",
    "print(\"query_cross_attention shape {}\".format(query_cross_attention.shape) )\n",
    "\n",
    "context_vector_cross_attention = query_cross_attention.T.matmul(F.softmax(keys_cross_attention.matmul(values_cross_attention.T)/k**0.5, dim=0))\n",
    "print(\"context_vector_cross_attention shape {}\".format(context_vector_cross_attention.shape) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
